---
categories:
- Mathematics
# - Programming
# - Phase Field
# - Others
tags:
- Linear Algebra
- Note
title: 张量代数笔记 II 
description: 线性代数2-线性映射与矩阵
date: 2025-10-10T13:29:06+08:00
image: 
math: true
hidden: false
comments: true
draft: true
---

$$
\gdef\dd{\space\mathrm{d}}
\gdef\p{\partial}
\gdef\Hom{\operatorname{Hom}}
\gdef\End{\operatorname{End}}
\gdef\Mat{\mathcal{M}}
\gdef\diag{\operatorname{diag}}
\gdef\GL{\operatorname{GL}}
\gdef\SL{\operatorname{SL}}
\gdef\O{\operatorname{O}}
\gdef\SO{\operatorname{SO}}
\gdef\tr{\operatorname{tr}}
\gdef\adj{\operatorname{adj}}
\gdef\inv{^{-1}}
\gdef\trans{^{\top}}
\gdef\pfrac#1#2{\dfrac{\p #1}{\p #2}}
\gdef\ddfrac#1#2{\dfrac{\mathrm{d} #1}{\mathrm{d} #2}}
\gdef\tensor#1{\boldsymbol{#1}}
$$

## 前言

本节我们将目光专注到矩阵上。线性代数从某种角度来说就是在研究矩阵以及其运算，或者说正是矩阵上的丰富代数结构才使线性代数称为这样一门应用广泛的课程。

## 矩阵的代数结构

矩阵上面可以定义丰富的代数结构。矩阵本身可以组成线性空间，而特殊的矩阵配以已有的或新的运算之后，可以生成更复杂精巧的代数结构。

### 矩阵构成的线性空间

如第一篇所述，$n$ 维线性空间 $V$ 到 $m$ 维线性空间 $W$ 的全体线性映射 $\Hom(V,W)$ 在配备逐点的标量乘法后形成一个线性空间 $\Mat(m,n)$。那么它的基是什么样的？我们给出它最典型的一组基，即令第 $(i,j)$ 个元素是 $1$ 而其他所有元素都为 $0$。不难验证它确实是一组基，且这组基让我们确定了一个 $m\times n$ 矩阵空间，它的一组基拥有 $mn$ 个基向量，因此也是 $mn$ 维线性空间。

然而这样的线性空间，它上面的运算太单薄，因此我们更喜欢的是 $n$ 阶方阵组成的线性空间。

### \$n\$ 阶方阵构成的代数结构

$n$ 维线性空间 $V$ 到自身的线性映射（即 $V$ 的线性变换）的全体可以记为 $\End(V)$，其在选定基后可以表示为 $n$ 阶方阵 $\Mat(n)$。它除了是一个线性空间之外，它还具有一些别的更好的性质，允许我们定义更多的运算，形成更复杂的结构。

#### 矩阵环

在集合 $\Mat(n)$ 上定义乘法为矩阵乘法，定义加法为逐点加法，则 $\Mat(n)$ 形成一个环，称为 **矩阵环**。它含有幺元，即单位阵；也有零元，即零矩阵。注意这样的结构上没有标量乘法，也就是说我们不能有一个数字乘以一个矩阵的运算，这是不合法的。

然而我们经常用数字乘以矩阵，这是由于我们给它进一步赋予了这个运算，使之成为了更复杂的数学结构。

#### 矩阵代数

在矩阵环 $\Mat(n)$ 的基础上定义标量乘法，与矩阵乘法的前后两个矩阵都满足线性性，则成为 **矩阵代数**，其是一个结合代数（由于矩阵乘法满足结合律）。

除了通过在环上配备标量乘法形成代数这个方法外，我们还可以通过给矩阵形成的线性空间 $\Mat(n)$ 赋予向量乘法，让他成为一个代数。这两个路径定义出的矩阵代数是同一个结构。

#### 李代数

除了定义普通乘法，我们还可以定义李括号 $[\tensor{A},\tensor{B}] = \tensor{AB-BA}$，则 $\Mat(n)$ 进一步形成一个李代数。李代数在微分几何中有广泛的应用，其与李群一起称为研究连续变化的几何体的便利工具。不过这里我们仅提及它，作为矩阵空间上能形成的代数结构。

### 可逆矩阵构成的代数结构

当我们把矩阵进一步规约到可逆矩阵上，则它能形成更复杂更精巧的代数结构

#### 一般线性群和特殊线性群

$n$ 阶可逆方阵全体，记作 $\GL(n)$ 与矩阵乘法构成 **一般线性群**。特别地，若要求方阵行列式为 $1$，则构成 **特殊线性群** $\SL(n)$。一般线性群代表了所有 $n$ 维线性空间之间的同构，因此有特别的意义，通过研究它，可以进一步了解线性空间的结构，且我们可以使用很多来自群论的工具。

而如果我们给线性空间定义内积，我们就可以让上面的一些矩阵成为所谓的正交矩阵。

#### 正交群（旋转群）

$n$ 阶正交矩阵 $\O(n)$ 与矩阵乘法形成 **正交群**，特别地，要求正交矩阵的行列式为 $1$ （而非 $-1$）时，得到 **特殊正交群** $\SO(n)$。$\SO(n)$ 为连续群，可以配以拓扑结构后形成一个 **李群**。李群要求群乘法和乘法的逆映射是光滑映射。这里给两个群的拓扑结构继承自 $\R^{n\times n}$。

此外还有一些矩阵群，如酉群/特殊酉群（来源于酉矩阵，指矩阵的共轭转置等于逆矩阵），这里不再介绍。

## 坐标变换

由于张量中会大量应用坐标变换，我们这里特别讨论这个问题。

### 一般的线性映射

我们再来看一般的线性映射，从线性映射如何表达为一个矩阵出发，观察矩阵对向量/矩阵的分量有什么样的影响。

设有从 $n$ 维线性空间 $V$ 到 $m$ 维线性映射 $W$ 的线性映射 $\tensor{T}$，在选定两个空间中的基后，该线性映射表达为 $[T^i{}_j]$，$1\leq i\leq m$，$1\leq j\leq n$，其第 $j$ 列为 $V$ 中的第 $j$ 个基在映射到 $W$ 后在 $W$ 中的基表示，矩阵有 $n$ 列表示了 $V$ 空间的基包含 $n$ 个线性无关的向量。

在这样的情况下，设有向量 $\tensor{v}\in V$，它除了可以直接在线性映射 $\tensor{T}$ 下成为 $w$ 中的向量之外，根据线性映射的特点，还可以先通过表达为 $V$ 中的基线性组合，将 $V$ 中的基映射到 $W$ 中后得到新的一组向量，最后再按照原有的线性组合方式（原来的列向量分量）重新组合得到 $\tensor{v}$ 在 $\tensor{T}$ 下的像 $\tensor{w}\in W$。这样能形成一个类似于交换图的结构，即直接映射的路径，它和先拆分为基向量线性组合，然后再映射基向量，最后将映射后的结果重新线性组合的路径得到的结果是一模一样的。

我们仔细研究这个过程。设 $V$ 中的有序基向量为 $\{\tensor{e}_i\}$，$W$ 中的有序基向量为 $\{\tensor{\varepsilon}_j\}$，这里的 $i$ 仅代表变化范围。此时我们可以讨论 $V$ 中的基向量在映射到 $W$ 后在 $\{\tensor{\varepsilon}_j\}$ 下的表示。由 $\tensor{T}$ 中元素的定义，我们可以得到，矩阵 $\tensor{T}$ 的第 $j$ 列即为 $\tensor{e}_j$ 在映射到 $W$ 空间后在 $W$ 的基 $\{\tensor{\varepsilon}_j\}$ 中表示出来的线性组合系数。即：

$$\tensor{T}(\tensor{e}_j) = \sum_{i}^{m} T^i{}_j \tensor{\varepsilon}_i = T^1{}_j \tensor{\varepsilon}_1 + \cdots + T^m{}_j \tensor{\varepsilon}_m,$$

左边表示 $V$ 中第 $i$ 个基向量 $\tensor{e}_i$ 在 $\tensor{T}$ 下映射的结果，右侧则是它的线性组合，其中 $T^i{}_j$ 是组合系数。这里我们将组合系数放在了基向量前面。这个式子完全地捕捉了上面的信息，即：

> [!REM]{基的映射}
>
> 线性空间的基在线性变换下的结果将表示为陪域上的基的线性组合，这个组合系数由线性映射的矩阵表示唯一确定，矩阵的第 $j$ 列即为定义域中第 $j$ 个基应该在陪域使用的组合系数列表，其第 $i$ 个分量代表了陪域中第 $i$ 个基应该数乘的系数。

那么，对于 $V$ 中的任意向量 $\tensor{v}$ 以及其在 $\tensor{T}$ 下的像，我们有

$$\tensor{Tv} = \tensor{T}(\sum_j^n v^j \tensor{e}_j) = \sum_j^n v^j\tensor{T}(\tensor{e}_j) = \sum_j^n \sum_i^m v^j T^i{}_j \tensor{\varepsilon}_i,$$

上面的式子里，第一个等号是 $\tensor{v}$ 的线性表出，第二个等号源自于线性映射的线性性，第三个等号则是单纯按照我们已有的结论展开。我们对求和换一下顺序，并利用 **实数的乘法交换律**，得到这样的结果：

$$ \tensor{Tv} = \sum_i^m\sum_j^nT^i{}_jv^j \tensor{\varepsilon}_i.$$

这个式子正说明了 $\tensor{Tv}$ 在 $[\tensor{\varepsilon}_i]$ 下的应该如何表示。同时说明了矩阵乘法实际上是在计算这样的东西：在原空间的向量的线性表出系数，在向量被映射入新空间后，其像在新空间的基下的线性表出系数。

### 同维线性空间的线性映射

现在我们对要研究的线性映射提出进一步的要求，它的陪域和定义域现在具有相同的维数。同样，我们给二者各自选择一组基，以表示两个空间中的向量。利用上面的讨论，我们得到类似的结论，但是此次的矩阵成为一个方阵。

我们指出，两个向量空间维度相同，则它们之间存在线性同构，且所有的 $n$ 维实线性空间都同构于 $\R^n$。借助这样的特点，我们可以将两个 $n$ 维线性空间之间的的线性映射看作 $\R^n$ 上的一个自同态。这样一来，我们可以给矩阵 $\tensor{T}$ 赋予新的不同的含义。

继续使用我们刚刚讨论过的方式，但是现在将两个空间的基都选为 $\R^n$ 上的标准正交基，即沿着每个坐标轴方向的单位向量组成的基。此时，由于原空间和新空间共用一套基，这个线性映射在表达为一个矩阵后，其第 $j$ 列的含义为 *向量空间中的第 $j$ 个基向量在经过一次映射后，其在原基下的表达（组合系数）*。

这样的特点让我们可以讨论两个问题：

- 原向量空间中的向量（我们提前称为 *协变向量*）在经过线性映射后，会 **随着线性空间一起** 被映射到哪里；
- 空间中的向量（我们提前称为 *逆变向量*）在选定的基发生变化后，会 **重新被表为哪些系数**。

我们分开来讨论这两种情况。

<!--  *只将向量映射到该空间的一个新的向量中*。我们仔细考察这个过程。

当我们将 $\R^n$ 的一组基通过线性映射 $\tensor{T}$ 映射回 $\R^n$ 自身时，根据我们在一般线性映射中讨论过的结果，$\tensor{T}$ 的列向量有这样的意义：它是定义域 $\R^n$ 的一个基在经过 $\tensor{T}$ 映射后再次表达为陪域 $\R^n$ 的基的线性组合时，应该使用的线性组合的系数。而由于两个线性空间实则为同一个空间且使用同一组基，我们得出结论：线性映射 $\tensor{T}$ 此时实际上记录了基在经过映射后，在原来的基下表达的新坐标。我们将这个情况推广到一般向量中，借助同态的性质，经过相似的步骤，我们得到相似的结论，即矩阵乘法具有和以前相似的含义：记录了向量的坐标应该如何随着线性空间变化而变化。

如此，我们可以讨论这样的两个问题：线性空间本身发生线性的变化，或者更换线性空间的基，其上的向量和线性有映射的描述会发生什么样的变化。 -->

#### 协变向量的变化

根据我们对协变向量的描述，我们知道，它和我们在 “一般的线性映射” 中谈论的向量映射结果是类似的。即：向量先分解为基向量的线性组合，然后线性空间的基被线性映射映射到新的向量上，最后照原方式对新向量组们进行线性组合。而由于现在我们的向量是 $\R^n$ 中的一个数表，它在基下的线性组合系数已经由数表形式的列向朗给出了，所以第一步是已经自然完成的；由于矩阵在线性映射中的意义已经给出了基向量被映射后的结果了，第二步也是给出了的；我们现在只需要做第三步，用列向量做系数来组合矩阵中表达出来的向量组。我们指出这个结果就是矩阵和列向量的乘法。


### 可逆变换的情况

设有可逆变换 $\tensor{P}$ 表示 $V$ 到 $V$ 的自同构。变换前的基为 $\{\tensor{e}_1,\tensor{e}_2,\cdots,\tensor{e}_n\}$，其组成的矩阵在原基下表达为单位阵 $\tensor{I}$；变换后的基为 $\{\tensor{\varepsilon}_1,\tensor{\varepsilon}_2,\cdots,\tensor{\varepsilon}_n\}$，它们在原基下表达即组成 $\tensor{P}$ 的列向量们：$\tensor{P} = [\tensor{p}_i] = [\tensor{\varepsilon}_i]$ ，而在新基下的表达也是 $\tensor{I}$。由此，我们得到，原基中的向量在新基下表达为矩阵 $P$ 的逆矩阵中的每个列向量。

### 向量的坐标变换

向量 $\tensor{v}$ 在变换 $\tensor{P}$ 的变换下得到 $\tensor{v'}$，$\tensor{v}$ 在旧基下表达为 $\sum_i v^i\tensor{e}_i$，由于基变换，我们得知


$$
\tensor{v'} = \tensor{Pv},
$$


## 对偶空间，多重线性映射

> [!DEF]{对偶空间}
>
> 一个线性空间 $V$ 的 **对偶空间** 记作 $V^*$，其元素为从 $V$ 到 $\R$ 的线性映射，其同样满足线性空间的八个性质。我们称线性空间 $V$ 中的元素为 *向量*，则称其对偶的线性空间中的元素为 **余***向量*。取一个余向量 $v^*$，则它是一个从 $V$ 到 $\R$ 上的函数，意味着 $v^*(w)$ 是一个实数，如果我们让 $v^* \in V^*$，$w\in V$。另外，我们还可以认为 *向量是余线性空间到 $\R$ 的函数*，因为给定一个 $v \in V$ 后，任何一个 $w^* \in V^*$ 都可以和 $v$ 一起运算得到一个值。


## 张量

我们先介绍张量的定义，得到定义后我们会介绍若干种张量最基础的代数运算，例如加减乘除等等。

### 张量的定义

张量其实有若干种定义方式。我们这里给出两种，一种用来方便运算，而另一种便于理解张量在运算过程中的一些性质。

> [!DEF]{张量（多维数表）}
>
> 张量可以在一个选定的坐标系下被表示为一个多维数组，而当变换坐标或者变换描述用的基底时，根据其 “逆变性” 与 “协变性”，该多维数表中的对应分量会按规则进行变化。不随坐标系改变或描述方法改变其几何性质的分量为 *逆变分量*，会随着坐标系改变而改变的分量则称为 *协变分量*。

这个定义是最实在的一种，没有什么花里胡哨的东西，下面给一些这个定义下的例子

> [!EX]
> 
> - 如果一个张量只有一个逆变分量或只有一个协变分量，其即为 *一阶张量*，成为一个向量；逆变的是 *列向量*，协变的是 *行向量*。我们把逆变指标放在上面，协变指标放在下面，比如 $\tensor{v}^i$ 是列向量，$\tensor{u}_j$ 是行向量。
> - 如果一个张量有两个分量，则它是一个矩阵，有四种情况：${T}_{ij}$，$T_i{}^j$，$T^i{}_j$ 和 $T^{ij}$。其中第一个和最后一个称为二次型，而第二和第三个称为线性映射，它们都可以用矩阵表示出来，其区别在于可以作用的量有所不同，后续会介绍。

我们知道，矩阵和矩阵、矩阵和向量、向量和向量之间都有丰富的运算，这些我们到后面再做介绍。下面我们介绍张量的一个比较数学化、形式化的定义：


## 线性变换和矩阵的几何意义

我们再次考察矩阵和线性变换，这一次从其几何角度考虑矩阵的含义。我们只考虑方阵的情况，并令其代表从线性空间 $V$ 到其自身的线性变换。

### 矩阵与向量

矩阵作用在一个向量上，可以将一个向量映射到另一个向量；若向量为矩阵的特征向量，则新向量的方向与原方向相同，只有大小不同：大小为特征向量。因此，特征向量代表 *线性变换下不改变方向的向量*，而特征值则代表这些特征向量在线性变换下的长度该变量。

### 矩阵作为空间的线性变换

一个方阵作为线性变换时，可以视为将基向量映射到某些其他向量上，矩阵的行列式为线性空间中的 **体积元** 的变化率，迹为线性空间中 **体积微元** 的变化率。若矩阵的行列式为 $0$，则新线性空间的维数一定小于原线性空间，新线性空间的维数由矩阵的秩给出。

### 可逆矩阵的几何意义

#### 一般可逆矩阵

可逆矩阵表达了从线性空间到自身的同构，其不改变线性空间本身的性质，只改变了描述线性空间的基。

#### 正交矩阵

正交矩阵的特点要求其一定将一组正交归一基映射到另一组正交归一基。这样一来，正交矩阵表示对线性空间的旋转和反射。若行列式为 $1$ 则表示其不改变映射前后的方向，而若行列式为 $-1$ 则表示改变映射前后的体积元方向。
